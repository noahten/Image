{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "robot.txtによるクロールの確認(reppyはインストール不可？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from reppy.robots import Robots\n",
    "import requests\n",
    "# rp = urllib.robotparser.RobotFileParser()\n",
    "# rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n",
    "# rp.read()\n",
    "# rrate = rp.request_rate(\"*\")\n",
    "# rrate.requests\n",
    "# rrate.seconds\n",
    "# rp.crawl_delay(\"*\")\n",
    "# rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\")\n",
    "# #False\n",
    "# rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n",
    "# #True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "from urllib import robotparser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "AGENT_NAME = 'PyMOTW'\n",
    "URL_BASE = 'https://ja-jp.facebook.com//'\n",
    "parser = robotparser.RobotFileParser()\n",
    "parser.set_url(urllib.parse.urljoin(URL_BASE,'robots.txt'))\n",
    "parser.read()\n",
    "\n",
    "PATHS = [\n",
    "    '/',#ホームページのrobot\n",
    "    '/PyMOTW/',#ホームページの相対パスを書く\n",
    "    '/admin/',\n",
    "    '/downloads/',\n",
    "    ]\n",
    "\n",
    "for path in PATHS:\n",
    "    print ('%6s : %s' % (parser.can_fetch(AGENT_NAME, path), path))\n",
    "    url = urllib.parse.urljoin(URL_BASE, path)\n",
    "    print ('%6s : %s' % (parser.can_fetch(AGENT_NAME, url), url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleMap から周辺情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "住所は千葉県　野田市\n"
     ]
    },
    {
     "ename": "GeocoderError",
     "evalue": "Error OVER_QUERY_LIMIT\nQuery: https://maps.google.com/maps/api/geocode/json?address=%E5%8D%83%E8%91%89%E7%9C%8C%E3%80%80%E9%87%8E%E7%94%B0%E5%B8%82&sensor=false&bounds=&region=&language=&components=",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGeocoderError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cded1854260f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mserch_word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'住所は'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maddress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserch_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGeocoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\noaht\\Anaconda3\\lib\\site-packages\\pygeocoder.py\u001b[0m in \u001b[0;36mgeocode\u001b[1;34m(self, address, sensor, bounds, region, language, components)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mGeocoderResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mGeocoderResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGeocoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0momnimethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\noaht\\Anaconda3\\lib\\site-packages\\pygeocoder.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mGeocoderError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG_GEO_OK\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mGeocoderError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'status'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'results'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mGeocoderError\u001b[0m: Error OVER_QUERY_LIMIT\nQuery: https://maps.google.com/maps/api/geocode/json?address=%E5%8D%83%E8%91%89%E7%9C%8C%E3%80%80%E9%87%8E%E7%94%B0%E5%B8%82&sensor=false&bounds=&region=&language=&components="
     ]
    }
   ],
   "source": [
    "from pygeocoder import Geocoder\n",
    "serch_word=input('住所は')\n",
    "address = str(serch_word)\n",
    "results = Geocoder.geocode(address)\n",
    "print(results[0].coordinates)\n",
    "\n",
    "result = Geocoder.reverse_geocode(*results.coordinates, language=\"ja\")\n",
    "\n",
    "\n",
    "from googleplaces import GooglePlaces, types, lang\n",
    "\n",
    "YOUR_API_KEY ='AIzaSyDqPooUGQGBImbnHHRxvjxL6QpuZpze0ac'\n",
    "\n",
    "google_places = GooglePlaces(YOUR_API_KEY)\n",
    "\n",
    "# You may prefer to use the text_search API, instead.\n",
    "query_result = google_places.nearby_search(\n",
    "        location=results, keyword='TOHOシネマズ',\n",
    "        radius=10000)#, types=\"MOVIE\")#movie_theater\n",
    "# If types param contains only 1 item the request to Google Places API\n",
    "# will be send as type param to fullfil:\n",
    "# http://googlegeodevelopers.blogspot.com.au/2016/02/changes-and-quality-improvements-in_16.html\n",
    "\n",
    "if query_result.has_attributions:\n",
    "    print (query_result.html_attributions)\n",
    "\n",
    "\n",
    "for place in query_result.places:\n",
    "    # Returned places from a query are place summaries.\n",
    "    print (place.name)\n",
    "    print (place.geo_location)\n",
    "    print (place.place_id)\n",
    "\n",
    "    # The following method has to make a further API call.\n",
    "    place.get_details()\n",
    "    # Referencing any of the attributes below, prior to making a call to\n",
    "    # get_details() will raise a googleplaces.GooglePlacesAttributeError.\n",
    "    #print (place.details) # A dict matching the JSON response from Google.\n",
    "    print (place.local_phone_number)\n",
    "    print (place.international_phone_number)\n",
    "    print (place.website)\n",
    "    print (place.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQLに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pymysql.cursors\n",
    "import MySQLdb\n",
    "\n",
    "\n",
    "\n",
    "conn=MySQLdb.connect(host=\"localhost\",\n",
    "                          user='root',\n",
    "                          password='',\n",
    "                          db='1113mysql',\n",
    "                          charset='utf8',\n",
    "                    )\n",
    "\n",
    "\n",
    "url = 'https://api.flickr.com/services/rest/'\n",
    "API_KEY = '25b04bcb3cad7ed64fd7f278676dea3a'\n",
    "SECRET_KEY = 'e970d4a9b95367d5'\n",
    "page=5\n",
    "serchword='sky'\n",
    "\n",
    "query = {\n",
    "        'method': 'flickr.photos.search',\n",
    "        'api_key': API_KEY,\n",
    "        'text': serchword,  #検索ワード\n",
    "        'per_page': page,  #1ページ辺りのデータ数\n",
    "        'format': 'json',\n",
    "        'nojsoncallback': '1'\n",
    "        }\n",
    "\n",
    "r = requests.get(url, params=query)\n",
    "#ファイルへの書きこみ\n",
    "writer=json.dumps(r.json(), sort_keys=True, indent=2)\n",
    "f = open('flickr.json', 'w') \n",
    "f.writelines(writer) \n",
    "f.close()\n",
    "#ファイルの出力\n",
    "f=open(\"flickr.json\", \"r\",encoding=\"utf-8-sig\")\n",
    "js=json.loads(f.read())\n",
    "f.close()\n",
    "\n",
    "#https://farm{farm}.staticflickr.com/{server}/{id}_{secret}_{size}.jpg'\n",
    "\n",
    "s=\"URL: \\n\"\n",
    "for z in js[\"photos\"][\"photo\"]:\n",
    "    s +=\"https://farm\"+str(z[\"farm\"]) + \".staticflickr.com/\" + z[\"server\"] + \"/\" + z[\"id\"] + \"_\" + z[\"secret\"] + \"_b.jpg\"+ \"\\n\"\n",
    "\n",
    "\n",
    "urllist=(s.split('\\n'))\n",
    "urllist.pop(0)\n",
    "print(urllist)\n",
    "\n",
    "#カーソルの取得\n",
    "cur = conn.cursor()\n",
    "#データベースを作る\n",
    "cur.execute('DROP DATABASE IF EXISTS flickr')\n",
    "cur.execute('CREATE DATABASE flickr')\n",
    "#データベースを変える\n",
    "cur.execute('USE flickr')\n",
    "#テーブルの作成\n",
    "#execute()でSQL文を実行する\n",
    "cur.execute('DROP TABLE IF EXISTS items')\n",
    "cur.execute('''\n",
    "        CREATE TABLE items(\n",
    "        id integer,\n",
    "        URL text\n",
    "    )     \n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "#データの挿入\n",
    "#cur.execute('INSERT INTO items VALUES(%s,%s)', (1,'URL'))    \n",
    "\n",
    "cur.executemany('INSERT INTO items VAlUES (%(id)s, %(URL)s)',[\n",
    "                {'id':1,'URL':str(urllist[0])},\n",
    "                {'id':2,'URL':str(urllist[1])},\n",
    "                {'id':3,'URL':str(urllist[2])},\n",
    "                {'id':4,'URL':str(urllist[3])},\n",
    "                {'id':5,'URL':str(urllist[4])},\n",
    "                ])\n",
    "#commitで保存\n",
    "conn.commit()\n",
    "#データの抽出\n",
    "cur.execute('SELECT * FROM items')\n",
    "for row in cur.fetchall():\n",
    "    print(row)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YouTubeに情報を送信PV取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import requests\n",
    "import json \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from apiclient.discovery import build  # pip install google-api-python-cliet\n",
    "\n",
    "YOUTUBE_API_KEY = ['AIzaSyCYneLWyBSxGdJHxVuyBElhVK5mEnfcEjs']  # 環境変数からAPIキーを取得する。os.environ\n",
    "url='https://www.googleapis.com/auth/youtube.readonly'\n",
    "\n",
    "# YouTubeのAPIクライアントを組み立てる。build()関数の第1引数にはAPI名を、\n",
    "# 第2引数にはAPIのバージョンを指定し、キーワード引数developerKeyでAPIキーを指定する。\n",
    "# この関数は、内部的に https://www.googleapis.com/discovery/v1/apis/youtube/v3/rest という\n",
    "# URLにアクセスし、APIのリソースやメソッドの情報を取得する。\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "# キーワード引数で引数を指定し、search.listメソッドを呼び出す。\n",
    "# list()メソッドでgoogleapiclient.http.HttpRequestオブジェクトが得られ、\n",
    "# execute()メソッドを実行すると実際にHTTPリクエストが送られて、APIのレスポンスが得られる。\n",
    "query = youtube.search().list(\n",
    "    part='snippet',\n",
    "    q='ちはやふる',\n",
    "    type='video',\n",
    ").execute()\n",
    "\n",
    "#r = requests.get(url,params=query)\n",
    "#print(r)\n",
    "print(json.dumps(query,sort_keys=True , indent=4))\n",
    "#print (json.dumps(query.json(),sort_keys=True, indent=2))\n",
    "\n",
    "# search_responseはAPIのレスポンスのJSOqqNをパースしたdict。\n",
    "print('タイトル')\n",
    "for item in query['items']:\n",
    "    print(item['snippet']['title'])  # 動画のタイトルを表示する。\n",
    "print('概要欄')\n",
    "for description in query['items']:\n",
    "    print(description['snippet']['description']) #動画の概要欄を表示\n",
    "    \n",
    "print('チャンネルタイトル')\n",
    "for description in query['items']:\n",
    "    print(description['snippet']['channelTitle']) #動画の概要欄を表示\n",
    "    \n",
    "    if (description['snippet']['channelTitle'] == '東宝MOVIEチャンネル'):\n",
    "        print('映画へ')\n",
    "\n",
    "\n",
    "print('サムネイル')\n",
    "for thumbnails in query['items']:\n",
    "    print(thumbnails['snippet']['thumbnails']['default']['url']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "東宝　https://hlo.tohotheater.jp/net/movie/TNPI3090J01.do　から　入手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESTINY 鎌倉ものがたり', '劇場版総集編アニメ『刀剣乱舞-花丸-』～幕間回想録～', '火花', 'GODZILLA 怪獣惑星', 'ラストレシピ ～麒麟の舌の記憶～', 'ミックス。', 'ナラタージュ', '亜人', '映画 あさひなぐ', '忍びの国', '映画 妖怪ウォッチ シャドウサイド 鬼王の復活', '未成年だけどコドモじゃない', '嘘を愛する女', '祈りの幕が下りる時', '空海―KU-KAI―', '映画ドラえもん のび太の宝島', '坂道のアポロン', 'ちはやふる －結び－', '映画 クレヨンしんちゃん 爆盛！カンフーボーイズ ～拉麺大乱～', '名探偵コナン ゼロの執行人（しっこうにん）', 'いぬやしき', 'となりの怪物くん', 'ラプラスの魔女', 'のみとり侍', '恋は雨上がりのように', 'OVER DRIVE', '羊と鋼の森', '劇場版ポケットモンスター 2018', '未来のミライ', '劇場版 コード・ブルー-ドクターヘリ緊急救命-（仮）', '僕のヒーローアカデミアTHE MOVIE', '検察側の罪人', 'SUNNY 強い気持ち・強い愛', '累-かさね-', '響-HIBIKI-', '散り椿', '億男', 'マスカレード・ホテル', 'GODZILLA2（仮）']\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# アクセスするURL\n",
    "url = \"https://www.toho.co.jp/movie/index.html\"\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "# htmlをBeautifulSoupで扱う\n",
    "soup = BeautifulSoup(response, \"html.parser\")#lxml\n",
    "\n",
    "#for a in soup.find_all('a'):\n",
    "#    print(a.get('href'))\n",
    "\n",
    "#for a in soup.find_all('a'):\n",
    "#    print(a.get('class'))\n",
    "\n",
    "#title_tag=soup.title\n",
    "#title = title_tag.string\n",
    "\n",
    "#print(title)\n",
    "\n",
    "#CSSセレクタによってタイトルを取得\n",
    "#現在上映している映画の題名取得\n",
    "import lxml.html\n",
    "import requests\n",
    "info=[]\n",
    " \n",
    "url =\"https://www.toho.co.jp/movie/lineup/index.html\"\n",
    "target_html = requests.get(url).content\n",
    "film= lxml.html.fromstring(target_html)\n",
    "i=0\n",
    "while True:\n",
    "    try:\n",
    "        info.append(film.cssselect('.thumb_ttl')[i].text_content())\n",
    "        i+=1\n",
    "    except:\n",
    "        break\n",
    "print(info)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#fp=\"https://www.toho.co.jp/movie/lineup/index.html\"\n",
    "#soup=BeautifulSoup(fp,\"html.parse\")\n",
    "\n",
    "#CSSセレクタで選び出す\n",
    "#print(soup.select_one(\"li:nth-of-type(8)\").string)\n",
    "#print(soup.select_one(\"#ve-list > li:nth-of-type(4)\").string)\n",
    "#print(soup.select(\"#ve-list > li[data-lo='us']\")[1].string)\n",
    "#print(soup.select(\"#ve-list > li.black\")[1].string)\n",
    "\n",
    "#findメソッドで選び出す\n",
    "#cond={\"data-lo\":\"us\",\"class\":\"black\"}\n",
    "#print(soup.find(\"li\",cond).string)\n",
    "\n",
    "\n",
    "#findメソッドを２度組み合わせる\n",
    "#print(soup.find(id=\"ve-list\").find(\"li\",cond).string)\n",
    "\n",
    "\n",
    "\n",
    "#CSS\n",
    "\n",
    "#上映が近い映画の題名取得\n",
    "# import lxml.html\n",
    "# import requests\n",
    "# info2=[]\n",
    " \n",
    "#url =\"https://www.toho.co.jp/movie/lineup/index.html#coming_soon\"\n",
    "#target_html = requests.get(url).content\n",
    "#comfilm = lxml.html.fromstring(target_html)\n",
    "#i=0\n",
    "#while True:\n",
    "#     try:\n",
    "#         info2.append(comfilm.cssselect('.thumb_ttl')[i].text_content())\n",
    "#         i+=1\n",
    "#         if (comfilm.cssselect('.span')[0].text_content()=='COMING SOON'):\n",
    "#             break\n",
    "#     except:\n",
    "#         break\n",
    "# print(info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# coding: UTF-8\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# アクセスするURL\n",
    "url = \"https://hlo.tohotheater.jp/net/schedule/035/TNPI2000J01.do\"\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "# htmlをBeautifulSoupで扱う\n",
    "soup = BeautifulSoup(response, \"html.parser\")#lxml\n",
    "#現在上映している映画の題名取得\n",
    "import lxml.html\n",
    "import requests\n",
    "info=[]\n",
    "##\\30 351-015418 > div.schedule-section-header.toggle-button.js-toggle-button.is-open > h5\n",
    "#リソースでは表示されないがデベロッパーツールでは表示される？\n",
    "url =\"https://hlo.tohotheater.jp/net/schedule/035/TNPI2000J01.do\"\n",
    "target_html = requests.get(url).content\n",
    "film= lxml.html.fromstring(target_html)\n",
    "#schedule-body-info\n",
    "i=0\n",
    "#p=soup.find_all('a')\n",
    "#print(p)\n",
    "\n",
    "h=soup.find(\"h5\", class_=\"schedule-body-title\")\n",
    "print(h)\n",
    "while i>8:\n",
    "    #try:\n",
    "    info.append(soup.find(\"h5\", class_=\"schedule-body-title\")[i].text_content())\n",
    "        #info.append(h)\n",
    "    print(info)\n",
    "    i+=1\n",
    "    #except:\n",
    "    #    break\n",
    "print(info)\n",
    "\n",
    "\n",
    "#CSSセレクタで検索する方法\n",
    "#sel=lambda q :print(soup.select_one(q).string)\n",
    "#sel(\"div> #en\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
